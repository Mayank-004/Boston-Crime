{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled15.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/Mayank-004/Boston-Crime/blob/main/BostonCRIME_Pyspark_Randomforest.ipynb",
      "authorship_tag": "ABX9TyPkCDVcFw1X7FxWzMF7hANa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayank-004/Boston-Crime/blob/main/BostonCRIME_Pyspark_Randomforest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Rvqhn6Kaqo",
        "outputId": "4208cec0-1f2a-4bfd-eb7f-717476c971a3"
      },
      "source": [
        "!pip install pyspark\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 68kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 39.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=3907545e3d05fd8ca9a71f2ea7e2012f760bae8dc265ad9cd449d962908b8f38\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LTbMHDatAMH"
      },
      "source": [
        "import pyspark\n",
        "import pandas as pd\n",
        "#from pyspark.sql import SparkSession\n",
        "#from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
        "#from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
        "#from pyspark.sql.functions import col,array_contains\n",
        "#fdf = spark.read.load(\"crime-incident-reports-2015.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaUlgRowAeoh"
      },
      "source": [
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "sc= SparkContext()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7gbjp1UwGBm"
      },
      "source": [
        "df1 = sqlContext.read.csv(\"/content/drive/MyDrive/Boston Crime Data/crime-incident-reports-2015.csv\",header=True)\n",
        "df2 = sqlContext.read.csv(\"/content/drive/MyDrive/Boston Crime Data/crime-incident-reports-2016.csv\",header=True)\n",
        "df3 = sqlContext.read.csv(\"/content/drive/MyDrive/Boston Crime Data/crime-incident-reports-2017.csv\",header=True)\n",
        "df4 = sqlContext.read.csv(\"/content/drive/MyDrive/Boston Crime Data/crime-incident-reports-2018.csv\",header=True)\n",
        "df5 = sqlContext.read.csv(\"/content/drive/MyDrive/Boston Crime Data/crime-incident-reports-2019.csv\",header=True)\n",
        "df6 = sqlContext.read.csv(\"/content/drive/MyDrive/Boston Crime Data/crime-incident-reports-2020.csv\",header=True)\n",
        "df6 = sqlContext.read.csv(\"/content/drive/MyDrive/Boston Crime Data/crime-incident-reports-2021.csv\",header=True)\n",
        "union_data = df1.unionAll(df2).unionAll(df3).unionAll(df4)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYtgSGVScMgv",
        "outputId": "17a4ae91-79b2-41ca-caf0-19f8596f2081"
      },
      "source": [
        "union_data.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+------------+-------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-------------+-----------+------------+--------------------+\n",
            "|INCIDENT_NUMBER|OFFENSE_CODE| OFFENSE_CODE_GROUP| OFFENSE_DESCRIPTION|DISTRICT|REPORTING_AREA|SHOOTING|   OCCURRED_ON_DATE|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|       STREET|        Lat|        Long|            Location|\n",
            "+---------------+------------+-------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-------------+-----------+------------+--------------------+\n",
            "|     I192068249|       02647|              Other|THREATS TO DO BOD...|      B2|           280|    null|2015-08-28 10:20:00|2015|    8|     Friday|  10|  Part Two|WASHINGTON ST|42.33011862|-71.08425106|(42.33011862, -71...|\n",
            "|     I192061894|       01106|   Confidence Games|FRAUD - CREDIT CA...|     C11|           356|    null|2015-08-20 00:00:00|2015|    8|   Thursday|   0|  Part Two|   CHARLES ST|42.30060543|-71.06126785|(42.30060543, -71...|\n",
            "|     I192038828|       01107|              Fraud|FRAUD - IMPERSONA...|      A1|           172|    null|2015-11-02 12:24:00|2015|   11|     Monday|  12|  Part Two|    ALBANY ST|42.33428841|-71.07239518|(42.33428841, -71...|\n",
            "|     I192008877|       01107|              Fraud|FRAUD - IMPERSONA...|     E18|           525|    null|2015-07-31 10:00:00|2015|    7|     Friday|  10|  Part Two|   WINGATE RD| 42.2370095|-71.12956606|(42.23700950, -71...|\n",
            "|     I182090828|       01102|              Fraud|FRAUD - FALSE PRE...|      D4|           159|    null|2015-12-01 12:00:00|2015|   12|    Tuesday|  12|  Part Two|     UPTON ST|42.34243222|-71.07225766|(42.34243222, -71...|\n",
            "|     I182090458|       01107|              Fraud|FRAUD - IMPERSONA...|     C11|           336|    null|2015-12-04 00:00:00|2015|   12|     Friday|   0|  Part Two|    NORTON ST|42.30626521|-71.06864556|(42.30626521, -71...|\n",
            "|     I182081063|       01107|              Fraud|FRAUD - IMPERSONA...|     E18|           478|    null|2015-12-01 12:25:00|2015|   12|    Tuesday|  12|  Part Two|RUSKINDALE RD|42.26889666| -71.1081375|(42.26889666, -71...|\n",
            "|     I182074094|       02629|         Harassment|          HARASSMENT|      B2|           258|    null|2015-09-14 09:31:00|2015|    9|     Monday|   9|  Part Two|  COLUMBIA RD|42.31514179|-71.06704709|(42.31514179, -71...|\n",
            "|     I182066132|       02629|         Harassment|          HARASSMENT|      B3|           455|    null|2015-07-31 23:27:00|2015|    7|     Friday|  23|  Part Two| RADCLIFFE ST|42.30020194|-71.07835353|(42.30020194, -71...|\n",
            "|     I182061268|       03201|      Property Lost|     PROPERTY - LOST|    null|              |    null|2015-06-15 00:00:00|2015|    6|     Monday|   0|Part Three|      BERNARD|         -1|          -1|(-1.00000000, -1....|\n",
            "|     I182054888|       02670|Criminal Harassment| CRIMINAL HARASSMENT|      B2|           326|    null|2015-07-12 15:37:00|2015|    7|     Sunday|  15|  Part Two|   FAYSTON ST|42.31224327|-71.07549901|(42.31224327, -71...|\n",
            "|     I182054888|       03170|              Other|INTIMIDATING WITNESS|      B2|           326|    null|2015-07-12 15:37:00|2015|    7|     Sunday|  15|Part Three|   FAYSTON ST|42.31224327|-71.07549901|(42.31224327, -71...|\n",
            "|     I182054888|       02647|              Other|THREATS TO DO BOD...|      B2|           326|    null|2015-07-12 15:37:00|2015|    7|     Sunday|  15|  Part Two|   FAYSTON ST|42.31224327|-71.07549901|(42.31224327, -71...|\n",
            "|     I182052842|       01102|              Fraud|FRAUD - FALSE PRE...|      D4|           149|    null|2015-12-20 14:00:00|2015|   12|     Sunday|  14|  Part Two| DARTMOUTH ST|42.34767067|-71.07620742|(42.34767067, -71...|\n",
            "|     I182052842|       00619|            Larceny|  LARCENY ALL OTHERS|      D4|           149|    null|2015-12-20 14:00:00|2015|   12|     Sunday|  14|  Part One| DARTMOUTH ST|42.34767067|-71.07620742|(42.34767067, -71...|\n",
            "|     I182044114|       01107|              Fraud|FRAUD - IMPERSONA...|     E18|           486|    null|2015-07-01 12:00:00|2015|    7|  Wednesday|  12|  Part Two|  OAKCREST RD|42.26452149|-71.10429211|(42.26452149, -71...|\n",
            "|     I182039429|       01107|              Fraud|FRAUD - IMPERSONA...|      C6|           226|    null|2015-11-26 08:00:00|2015|   11|   Thursday|   8|  Part Two|  E FOURTH ST|       null|        null|(0.00000000, 0.00...|\n",
            "|     I182032622|       02647|              Other|THREATS TO DO BOD...|      D4|           129|    null|2015-10-10 13:17:00|2015|   10|   Saturday|  13|  Part Two|  BERKELEY ST|42.34997636|-71.07242619|(42.34997636, -71...|\n",
            "|     I182031354|       01102|              Fraud|FRAUD - FALSE PRE...|     D14|           791|    null|2015-09-15 11:00:00|2015|    9|    Tuesday|  11|  Part Two|   ALLSTON ST|42.34438811| -71.1405858|(42.34438811, -71...|\n",
            "|     I182016943|       03201|      Property Lost|     PROPERTY - LOST|     C11|           366|    null|2015-08-20 08:00:00|2015|    8|   Thursday|   8|Part Three|     EDWIN ST|42.28943324| -71.0605219|(42.28943324, -71...|\n",
            "+---------------+------------+-------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-------------+-----------+------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "5BwJuHD2Kd_h",
        "outputId": "6cde0aff-f2e3-4569-a252-14c29ac3768a"
      },
      "source": [
        "union_data.describe().toPandas().transpose()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>summary</th>\n",
              "      <td>count</td>\n",
              "      <td>mean</td>\n",
              "      <td>stddev</td>\n",
              "      <td>min</td>\n",
              "      <td>max</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INCIDENT_NUMBER</th>\n",
              "      <td>353253</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>I010370257-00</td>\n",
              "      <td>I192077559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OFFENSE_CODE</th>\n",
              "      <td>353253</td>\n",
              "      <td>2318.6886650644155</td>\n",
              "      <td>1185.3273469951023</td>\n",
              "      <td>00111</td>\n",
              "      <td>3201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OFFENSE_CODE_GROUP</th>\n",
              "      <td>353253</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Aggravated Assault</td>\n",
              "      <td>Warrant Arrests</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OFFENSE_DESCRIPTION</th>\n",
              "      <td>353253</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>A&amp;B HANDS, FEET, ETC.  - MED. ATTENTION REQ.</td>\n",
              "      <td>WEAPON - OTHER - OTHER VIOLATION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DISTRICT</th>\n",
              "      <td>351426</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>A1</td>\n",
              "      <td>E5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>REPORTING_AREA</th>\n",
              "      <td>353253</td>\n",
              "      <td>384.54719403941095</td>\n",
              "      <td>241.8431942724011</td>\n",
              "      <td></td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SHOOTING</th>\n",
              "      <td>1455</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OCCURRED_ON_DATE</th>\n",
              "      <td>353253</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2015-06-15 00:00:00</td>\n",
              "      <td>2018-12-31 23:45:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>YEAR</th>\n",
              "      <td>353253</td>\n",
              "      <td>2016.6950174520812</td>\n",
              "      <td>1.036961986645097</td>\n",
              "      <td>2015</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MONTH</th>\n",
              "      <td>353253</td>\n",
              "      <td>6.958058388746876</td>\n",
              "      <td>3.3268346235849657</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DAY_OF_WEEK</th>\n",
              "      <td>353253</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HOUR</th>\n",
              "      <td>353253</td>\n",
              "      <td>13.098091169784828</td>\n",
              "      <td>6.298344900954269</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UCR_PART</th>\n",
              "      <td>353156</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Other</td>\n",
              "      <td>Part Two</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STREET</th>\n",
              "      <td>342048</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>ALBANY ST</td>\n",
              "      <td>ZELLER ST</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lat</th>\n",
              "      <td>330723</td>\n",
              "      <td>42.212353218559436</td>\n",
              "      <td>2.1794916840757637</td>\n",
              "      <td>-1</td>\n",
              "      <td>42.39504158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Long</th>\n",
              "      <td>330723</td>\n",
              "      <td>-70.90510702845228</td>\n",
              "      <td>3.5255374068055874</td>\n",
              "      <td>-1</td>\n",
              "      <td>-71.17867378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Location</th>\n",
              "      <td>353253</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>(-1.00000000, -1.00000000)</td>\n",
              "      <td>(42.39504158, -71.01017732)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          0  ...                                 4\n",
              "summary               count  ...                               max\n",
              "INCIDENT_NUMBER      353253  ...                        I192077559\n",
              "OFFENSE_CODE         353253  ...                              3201\n",
              "OFFENSE_CODE_GROUP   353253  ...                   Warrant Arrests\n",
              "OFFENSE_DESCRIPTION  353253  ...  WEAPON - OTHER - OTHER VIOLATION\n",
              "DISTRICT             351426  ...                                E5\n",
              "REPORTING_AREA       353253  ...                                99\n",
              "SHOOTING               1455  ...                                 Y\n",
              "OCCURRED_ON_DATE     353253  ...               2018-12-31 23:45:00\n",
              "YEAR                 353253  ...                              2018\n",
              "MONTH                353253  ...                                 9\n",
              "DAY_OF_WEEK          353253  ...                         Wednesday\n",
              "HOUR                 353253  ...                                 9\n",
              "UCR_PART             353156  ...                          Part Two\n",
              "STREET               342048  ...                         ZELLER ST\n",
              "Lat                  330723  ...                       42.39504158\n",
              "Long                 330723  ...                      -71.17867378\n",
              "Location             353253  ...       (42.39504158, -71.01017732)\n",
              "\n",
              "[18 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUMgYCKA9Ile"
      },
      "source": [
        "columns_to_drop = ['INCIDENT_NUMBER', 'OFFENSE_CODE','OFFENSE_CODE_GROUP','OFFENSE_DESCRIPTION','SHOOTING','OCCURRED_ON_DATE','Location']\n",
        "\n",
        "union_data_clean = union_data.drop(*columns_to_drop)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyuLhvap8A5"
      },
      "source": [
        "union_data_clean=union_data_clean.na.drop()\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DFw6UAC_2OV"
      },
      "source": [
        "union_data_clean=union_data_clean.fillna(0)\n"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0uvJ1AYAKBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee20d39-e269-4540-f827-83bd57b7a1c8"
      },
      "source": [
        "union_data_clean.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------+----+-----+-----------+----+----------+-------------+-----------+------------+\n",
            "|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|       STREET|        Lat|        Long|\n",
            "+--------+--------------+----+-----+-----------+----+----------+-------------+-----------+------------+\n",
            "|      B2|           280|2015|    8|     Friday|  10|  Part Two|WASHINGTON ST|42.33011862|-71.08425106|\n",
            "|     C11|           356|2015|    8|   Thursday|   0|  Part Two|   CHARLES ST|42.30060543|-71.06126785|\n",
            "|      A1|           172|2015|   11|     Monday|  12|  Part Two|    ALBANY ST|42.33428841|-71.07239518|\n",
            "|     E18|           525|2015|    7|     Friday|  10|  Part Two|   WINGATE RD| 42.2370095|-71.12956606|\n",
            "|      D4|           159|2015|   12|    Tuesday|  12|  Part Two|     UPTON ST|42.34243222|-71.07225766|\n",
            "|     C11|           336|2015|   12|     Friday|   0|  Part Two|    NORTON ST|42.30626521|-71.06864556|\n",
            "|     E18|           478|2015|   12|    Tuesday|  12|  Part Two|RUSKINDALE RD|42.26889666| -71.1081375|\n",
            "|      B2|           258|2015|    9|     Monday|   9|  Part Two|  COLUMBIA RD|42.31514179|-71.06704709|\n",
            "|      B3|           455|2015|    7|     Friday|  23|  Part Two| RADCLIFFE ST|42.30020194|-71.07835353|\n",
            "|    null|              |2015|    6|     Monday|   0|Part Three|      BERNARD|         -1|          -1|\n",
            "|      B2|           326|2015|    7|     Sunday|  15|  Part Two|   FAYSTON ST|42.31224327|-71.07549901|\n",
            "|      B2|           326|2015|    7|     Sunday|  15|Part Three|   FAYSTON ST|42.31224327|-71.07549901|\n",
            "|      B2|           326|2015|    7|     Sunday|  15|  Part Two|   FAYSTON ST|42.31224327|-71.07549901|\n",
            "|      D4|           149|2015|   12|     Sunday|  14|  Part Two| DARTMOUTH ST|42.34767067|-71.07620742|\n",
            "|      D4|           149|2015|   12|     Sunday|  14|  Part One| DARTMOUTH ST|42.34767067|-71.07620742|\n",
            "|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|  OAKCREST RD|42.26452149|-71.10429211|\n",
            "|      C6|           226|2015|   11|   Thursday|   8|  Part Two|  E FOURTH ST|       null|        null|\n",
            "|      D4|           129|2015|   10|   Saturday|  13|  Part Two|  BERKELEY ST|42.34997636|-71.07242619|\n",
            "|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|   ALLSTON ST|42.34438811| -71.1405858|\n",
            "|     C11|           366|2015|    8|   Thursday|   8|Part Three|     EDWIN ST|42.28943324| -71.0605219|\n",
            "+--------+--------------+----+-----+-----------+----+----------+-------------+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvV6IfiZHDKQ"
      },
      "source": [
        "union_data_clean = union_data_clean.withColumn(\"REPORTING_AREA\", union_data_clean['REPORTING_AREA'].cast('float'))\n",
        "union_data_clean = union_data_clean.withColumn(\"MONTH\", union_data_clean['MONTH'].cast('float'))\n",
        "union_data_clean = union_data_clean.withColumn(\"HOUR\", union_data_clean['HOUR'].cast('float'))\n",
        "union_data_clean = union_data_clean.withColumn(\"Lat\", union_data_clean['Lat'].cast('float'))\n",
        "union_data_clean = union_data_clean.withColumn(\"Long\", union_data_clean['Long'].cast('float'))\n",
        "#union_data_clean = union_data_clean.withColumn(\"REPORTING_AREA\", union_data_clean.call_time.cast('float'))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo-7gkzmKgne"
      },
      "source": [
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "indexers = [StringIndexer(inputCol=\"DISTRICT\", outputCol=\"DISTRICTIndex\") ,StringIndexer(inputCol=\"DAY_OF_WEEK\", outputCol=\"DAY_OF_WEEKIndex\") , StringIndexer(inputCol=\"UCR_PART\", outputCol=\"UCR_PARTIndex\"),StringIndexer(inputCol=\"STREET\", outputCol=\"STREETIndex\"),StringIndexer(inputCol=\"YEAR\", outputCol=\"YEARIndex\")]\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "union_data_clean = pipeline.fit(union_data_clean).transform(union_data_clean)\n",
        "\n",
        "#union_data_clean=indexers.setHandleInvalid(\"skip\").fit(union_data_clean).transform(union_data_clean)\n",
        "\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols = ['DISTRICTIndex','REPORTING_AREA','YEARIndex','MONTH','DAY_OF_WEEKIndex','HOUR','STREETIndex','Lat','Long'], outputCol = 'features')\n",
        "#vectorAssembler=vectorAssembler.transform(union_data_clean.na.drop)\n",
        "\n",
        "vunion_data = vectorAssembler.transform(union_data_clean)\n",
        "\n",
        "\n",
        "vunion_data_f = vunion_data.select(['features', 'UCR_PARTIndex'])\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhLSOiR-E-tA",
        "outputId": "281ff67a-5039-4033-de4f-bdd4c49bb0aa"
      },
      "source": [
        "vunion_data_f.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+\n",
            "|            features|UCR_PARTIndex|\n",
            "+--------------------+-------------+\n",
            "|[0.0,280.0,3.0,8....|          1.0|\n",
            "|[1.0,356.0,3.0,8....|          1.0|\n",
            "|[4.0,172.0,3.0,11...|          1.0|\n",
            "|[7.0,525.0,3.0,7....|          1.0|\n",
            "|[2.0,159.0,3.0,12...|          1.0|\n",
            "|[1.0,336.0,3.0,12...|          1.0|\n",
            "|[7.0,478.0,3.0,12...|          1.0|\n",
            "|[0.0,258.0,3.0,9....|          1.0|\n",
            "|[3.0,455.0,3.0,7....|          1.0|\n",
            "|[0.0,326.0,3.0,7....|          1.0|\n",
            "|[0.0,326.0,3.0,7....|          0.0|\n",
            "|[0.0,326.0,3.0,7....|          1.0|\n",
            "|[2.0,149.0,3.0,12...|          1.0|\n",
            "|[2.0,149.0,3.0,12...|          2.0|\n",
            "|[7.0,486.0,3.0,7....|          1.0|\n",
            "|[2.0,129.0,3.0,10...|          1.0|\n",
            "|[6.0,791.0,3.0,9....|          1.0|\n",
            "|[1.0,366.0,3.0,8....|          0.0|\n",
            "|[1.0,450.0,3.0,10...|          1.0|\n",
            "|[1.0,352.0,3.0,8....|          1.0|\n",
            "+--------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcWL3VP0KkIW"
      },
      "source": [
        "'''\n",
        "splits = vunion_data_f.randomSplit([0.9, 0.1])\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]\n",
        "'''\n"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BN-72-OPfBL"
      },
      "source": [
        "'''\n",
        "from pyspark.sql.functions import percent_rank\n",
        "from pyspark.sql import Window\n",
        "\n",
        "vunion_data_f = vunion_data_f.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"UCR_PARTIndex\")))\n",
        "train_df = vunion_data_f.where(\"rank <= .8\").drop(\"rank\")\n",
        "#train_df.show()\n",
        "test_df = vunion_data_f.where(\"rank > .8\").drop(\"rank\")\n",
        "#test_df.show()\n",
        "\n",
        "'''"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLH6Uy9QKnlZ"
      },
      "source": [
        "'''\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#vectorAssembler = VectorAssembler(inputCols = ['Cylinders','ManufactureIndex','ModelIndex','OriginIndex','Displacement','Weight'], outputCol = 'features')\n",
        "#vunion_data = vectorAssembler.transform(vunion_data)\n",
        "\n",
        "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'UCR_PARTIndex')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dt_model = dt.fit(train_df)\n",
        "\n",
        "dt_predictions_train = dt_model.transform(train_df)\n",
        "\n",
        "dt_predictions_test = dt_model.transform(test_df)\n",
        "dt_evaluator = RegressionEvaluator(labelCol=\"Horsepower\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse_train = dt_evaluator.evaluate(dt_predictions_train)\n",
        "rmse_test = dt_evaluator.evaluate(dt_predictions_test)\n",
        "\n",
        "print(\"Root Mean Squared Error (RMSE) on train data = %g\" % rmse_train)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse_test)\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kjS-rX-AUj-K",
        "outputId": "d7518153-a478-48f2-8b1a-dcb3673a9a45"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "(trainingData, testData) = vunion_data_f.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "#pipeline = Pipeline().setStages(indexers + [vectorAssembler,dt])\n",
        "\n",
        "dt = DecisionTreeClassifier(labelCol=\"UCR_PARTIndex\", featuresCol=\"features\")\n",
        "\n",
        "# Chain indexers and tree in a Pipeline\n",
        "#pipeline = Pipeline(stages=['UCR_PARTIndex', 'features', dt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = dt.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"UCR_PARTIndex\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"UCR_PARTIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "\n",
        "treeModel = model.stages[2]\n",
        "# summary only\n",
        "print(treeModel)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3b70c1429ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Train model.  This also runs the indexers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Make predictions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o803.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 127) (7c8b643e7425 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3214/0x000000084131f840: (struct<DISTRICTIndex:double,REPORTING_AREA_double_VectorAssembler_921fce7264d0:double,YEARIndex:double,MONTH_double_VectorAssembler_921fce7264d0:double,DAY_OF_WEEKIndex:double,HOUR_double_VectorAssembler_921fce7264d0:double,STREETIndex:double,Lat_double_VectorAssembler_921fce7264d0:double,Long_double_VectorAssembler_921fce7264d0:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1449)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1422)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.$anonfun$train$1(DecisionTreeClassifier.scala:135)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:114)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3214/0x000000084131f840: (struct<DISTRICTIndex:double,REPORTING_AREA_double_VectorAssembler_921fce7264d0:double,YEARIndex:double,MONTH_double_VectorAssembler_921fce7264d0:double,DAY_OF_WEEKIndex:double,HOUR_double_VectorAssembler_921fce7264d0:double,STREETIndex:double,Lat_double_VectorAssembler_921fce7264d0:double,Long_double_VectorAssembler_921fce7264d0:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1449)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 36 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqGFHEEFm6m_"
      },
      "source": [
        " pipeline = Pipeline().setStages(indexer + [assembler,dtc_model])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}